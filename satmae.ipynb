{"cells":[{"cell_type":"markdown","id":"b25486b4-b319-4f47-95b2-caa2ee0d9a2a","metadata":{"id":"b25486b4-b319-4f47-95b2-caa2ee0d9a2a"},"source":["# üéì Reproducing **Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery**  Paper\n","\n","Welcome to this **step-by-step guide** for **reproducing the results** of the paper **\"Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery\"**! This guide will walk through the process of setting up, running, and verifying the paper's experiments.  \n","\n","## üîπ Steps to Reproduce:  \n","1. **Clone the repository** ‚Äì Download the official code and set up the project.  \n","2. **Set up the environment** ‚Äì Install dependencies and configure necessary settings.  \n","3. **Download and preprocess datasets** ‚Äì Retrieve validation datasets and process them correctly.  \n","4. **Run experiments and reproduce results** ‚Äì Validate key figures or tables from the paper.  \n","\n","Let's get started and replicate the findings! üöÄ  "]},{"cell_type":"markdown","id":"ba28b366","metadata":{"id":"ba28b366"},"source":["### Summary of the paper:\n","The paper introduces SatMAE++, a new method to improve how transformer models learn from satellite images. The goal is to handle data from different sensors and scales better than older models like SatMAE, which don‚Äôt fully use all the available information. SatMAE++ uses a technique called multi-scale reconstruction to help the model learn details from images at different sizes. The method was tested on six datasets, including fMoW-RGB, fMoW-Sentinel, and BigEarthNet, and showed excellent results. It achieved a Top-1 Accuracy of 78.14% on fMoW-RGB. SatMAE++ also learns faster and performed better in land cover classification, with a 3.6% improvement, making it a strong tool for analyzing satellite images."]},{"cell_type":"markdown","id":"a838ba76","metadata":{"id":"a838ba76"},"source":["## Challenges Encountered:\n","\n","1. To speed up the process, I need access to a GPU. I created an account on the FAU Remote CIP Pool and set up a directory under my IDM ID.\n","2. I then began working on the project by cloning the GitHub repository.\n","3. In the Readme file, they did not mention anything in details like no information about environment setting, code run nothing at all specifically.\n","4. For the environment setup, I encountered challenges due to incomplete instructions in the README.md file. The Conda environment required significant storage, and I had to install  the required pip packages separately to address version compatibility issues.\n","5. Dataset github file mentioned but those links have some credential issues. So i need to do it manually with data preprocessing. See dataset section here.\n","6. To run the actual code, they just gave the command but not other informations where to change in code and which model they used.\n","7. Need to change in `mainfinetune.py`, `enginefinetune.py`, `misc.py` and `dataset.py` files. But the accuracy level is not appropiate like them.\n"]},{"cell_type":"markdown","id":"e21359fd-37e4-4d10-b3d0-43099b1634f4","metadata":{"id":"e21359fd-37e4-4d10-b3d0-43099b1634f4"},"source":["## üîπNo. 1: Clone the GitHub Repository üõ†Ô∏è\n","\n","The first step in reproducing the research paper is to clone the GitHub repository containing the code and resources from the paper. Below are the steps for cloning the repository to your **Remote machine**.\n","\n","### A. **Cloning on Your Remote Machine** üñ•Ô∏è\n","\n","1. Open your terminal on Visual Studio Prompt (Windows).\n","2. Go to the working folder.\n","\n","   ```zsh\n","    cd /proj/ciptmp\n","    cd ev72erij\n","\n","   ```\n","3. Step 1 will create a folder in my current directory with the same name as the repository (e.g., satmae_pp). You can navigate into the folder using:\n","   \n","   ```zsh\n","   cd satmae_pp\n","    \n","   \n","4. Run the following command to clone the repository:\n","\n","   ```zsh\n","   git clone https://github.com/techmn/satmae_pp\n","   \n","  This will create a local copy of the repository on FAU Remote machine.\n","\n","\n"]},{"cell_type":"markdown","id":"31f62acb-1a5e-4ef1-abb0-d715b862721e","metadata":{"id":"31f62acb-1a5e-4ef1-abb0-d715b862721e"},"source":["## üîπNo. 2: Set Up the Environment ‚öôÔ∏è\n","\n","After cloning the repository, the next step is to set up the environment where the code will run. This typically involves installing dependencies.\n","\n","While a requirements.txt file or cuda.yml are not provided. So need to create conda environment.\n","### Manually Resolving Dependencies üîß\n","\n","1. Run the following command in the terminal:\n","         ```\n","           conda env create satmaeenv\n","        ```\n","\n","  \n","2. Use pip to manually install the missing packages. For example:\n","   \n","   - With `pip`:\n","     ```zsh\n","     pip install tensorvision\n","     ```\n","\n","3. Repeat this process for any other missing or conflicting dependencies until the environment is successfully set up.\n","\n","> Since you are working on a **remote machine**, it is recommended to create a virtual environment (using venv or conda) to keep project dependencies isolated.\n","To activate the environment, run:\n","   ```zsh\n","      conda activate satmaeenv\n","   ```\n"]},{"cell_type":"markdown","id":"b4e02a8b-50e5-4251-a5fd-313cb8b83ae2","metadata":{"id":"b4e02a8b-50e5-4251-a5fd-313cb8b83ae2","outputId":"129c4176-455a-42b0-e06a-b30ef7792aea"},"source":["## üîπNo. 3: Reproducing the *Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery* Paper üéì\n","\n","\n","Link to Paper's Github Repo https://github.com/techmn/satmae_pp\n","\n","\n","\n","### üìÅ Dataset:\n","\n","Here, there are two dataset are available fMow-Sentinel and fMow-rgb. I am only working on FMow-RGB Dataset as the datasets are too large to work on.\n","In the paper, they are using distributed methods to run these datasets.\n","\n","### FMoW-RGB\n","You can download the dataset by following the instructions here [[fmow-github]](https://github.com/fMoW/dataset).\n","\n","**Issues:** There are some issues i faced when downloading the datasets.\n","As Aws/torrent links provided for download purpose, but torrent links aren't useable. Then for aws cls, there are some credential issue occured.\n","They put whole validation dataset into Six different parquet files. So need to write python script to set all data into one directory called **val**.\n","\n","### Data pre-processing\n","\n","Download validation set and need to pre-process it. For this, I need to write a python code. Save it to files.py and run it.\n","```cmd\n","   \n","\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import base64\n","\n","# Load the Parquet file\n","df = pd.read_parquet('val-00004-of-00005-c2208e03589db7a1.parquet')\n","\n","# Base output folder\n","output_base_folder = 'extracted_images'\n","os.makedirs(output_base_folder, exist_ok=True)\n","\n","# Iterate over each row and save the images\n","for index, row in tqdm(df.iterrows(), total=len(df)):\n","    try:\n","        filename = row['img_filename']\n","        file_path = row['img_path']  # Directory structure from file_path column\n","        \n","        if pd.isna(row['image']) or pd.isna(filename) or pd.isna(file_path):\n","            continue  # Skip rows with missing image data, filename, or file path\n","        \n","        # Handle different data types for image data\n","        if isinstance(row['image'], dict):\n","            image_data = row['image'].get('data') or row['image'].get('bytes')\n","        elif isinstance(row['image'], str) and row['image'].startswith('data:image'):\n","            image_data = base64.b64decode(row['image'].split(',')[1])\n","        else:\n","            image_data = row['image']\n","        \n","        # Validate byte data\n","        if not isinstance(image_data, (bytes, bytearray)):\n","            print(f\"Skipping {filename}: image data is not in byte format.\")\n","            continue\n","        \n","        # Extract directory path from file_path\n","        directory_path = os.path.join(output_base_folder, os.path.dirname(file_path))\n","        os.makedirs(directory_path, exist_ok=True)  # Ensure subdirectories exist\n","        \n","        \n","        full_file_path = os.path.join(directory_path, filename)\n","        \n","        # Save the image file\n","        with open(full_file_path, 'wb') as file:\n","            file.write(image_data)\n","    \n","    except Exception as e:\n","        print(f\"Failed to save {filename}: {e}\")\n","\n","print(\"All images have been saved successfully in their respective directories.\")\n","\n","```\n","For convenient, save this code in a file named file.py and run it in the command prompt.\n","\n","```cmd\n","python file.py\n","```\n","this code are done by me for pre-process the val datasets and save these into **val** folder.\n","\n","### üìÅ Directory Structure for Data Organization\n","\n","Directory structure of the dataset should look like as below:\n","\n","```\n","[Root folder]\n","____ train_62classes.json\n","____ val_62classes.json\n","____ train\n","________ aiport\n","________ aiport_hangar\n","________ .......\n","____ val\n","________ aiport\n","________ aiport_hangar\n","________ .......\n","```\n","\n","\n"]},{"cell_type":"markdown","id":"1da41a7d-4fd5-4030-8ee7-1d16f7bc545d","metadata":{"id":"1da41a7d-4fd5-4030-8ee7-1d16f7bc545d"},"source":["\n","#### üìù Explanation of the Structure:\n","\n","Download the train and validation json files [[data-split]](https://github.com/techmn/satmae_pp/tree/main/fmow_rgb_data_split).\n"]},{"cell_type":"markdown","id":"c03b9315-b664-4428-b76c-b8fd124b600c","metadata":{"id":"c03b9315-b664-4428-b76c-b8fd124b600c"},"source":["In this phase, walk through the steps to reproduce the results from the *Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery* paper using the cloned repository.\n","\n","## üîπNo 4: Run main experiments\n","\n","### Finetuning\n","Use the following command to finetune the ViT model (default is ViT-L). In the Readme.md file,\n","They gave command but need to edit it because of the requirements. So I edited it and put this command in `run.sh` to run easily.\n","\n","This is the edited command:\n","\n","CUDA_LAUNCH_BLOCKING=0 \\\n","CUDA_VISIBLE_DEVICES=0 python main_finetune.py \\\n","--batch_size 8 --accum_iter 16 \\\n","--epochs 50 --warmup_epochs 5 \\\n","--input_size 224 --patch_size 16 \\\n","--model_type vanilla \\\n","--dataset_type rgb \\\n","--weight_decay 0.05 --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n","--lr 0.001 --num_workers 16 \\\n","--train_path ./train_62classes.json \\\n","--test_path ./val_62classes.json \\\n","--output_dir ./finetune_dir \\\n","--log_dir ./finetune_dir \\\n","--eval \\\n","--cls_token \\\n","--resume ./checkpoint_ViT-L_finetune_fmow_rgb.pth\n","\n","\n","After put it to `run.sh`:\n","\n"," ```zsh\n","     /run.sh\n","```\n","\n","\n","------------------------------------------------------------------------------------"]},{"cell_type":"markdown","id":"e5d0375f-ee3a-480d-a8d5-0865921232b5","metadata":{"id":"e5d0375f-ee3a-480d-a8d5-0865921232b5","outputId":"e6df1ec7-fe5c-4b08-bee5-bcf077759c2a"},"source":["## Model Weights\n","Using ViT-L model for finetuning.\n","| Model | Dataset | Top1 Acc (%) |  Finetune |\n","| :---  |  :---:  |    :---:     |  :---:   |\n","| ViT-L | FMoW-RGB | 78.14 | [download](https://huggingface.co/mubashir04/checkpoint_ViT-L_finetune_fmow_rgb) |\n","\n"]},{"cell_type":"markdown","id":"93dcb18f-f3bc-4ee5-843e-f504f05cfc46","metadata":{"id":"93dcb18f-f3bc-4ee5-843e-f504f05cfc46"},"source":["## üìä Evaluation Results\n","\n","After running the evaluation command, the following result is obtained for only `fMow-rgb` dataset:\n","\n","![experiment.png](attachment:experiment.png)\n","\n"]},{"cell_type":"markdown","id":"8c08b1d6-2a06-40eb-99b6-af21bbe633c1","metadata":{"id":"8c08b1d6-2a06-40eb-99b6-af21bbe633c1"},"source":["## üìä Comparison of Evaluation Results\n","\n","### üìù Comparison with Paper's Results\n","\n","The expected results were not achieved in my experiment. Specifically, the Top-1 Accuracy obtained was `45.6%`, which is significantly lower than the original paper's reported value of `78.14%`. I believe this discrepancy is primarily due to issues related to poor code documentation. The lack of proper comments in the code made it challenging to understand the implementation details, which not only extended the execution time but also complicated the process of making necessary modifications. These factors likely contributed to the performance gap observed in the experimental results.\n","Below is the comparison of the evaluation results reported in the paper at `Table 2` for all the datasets.\n","\n","#### Paper Reported Results:\n","![paperpic.png](attachment:paperpic.png)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"xML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}